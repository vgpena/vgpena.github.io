<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hey it's violet</title>
  <subtitle>developer &amp; human in the PNW</subtitle>
  <id>https://vgpena.github.io/</id>
  <link href="https://vgpena.github.io"/>
  <link href="https://vgpena.github.io/feed.xml" rel="self"/>
  <updated>2018-12-28T15:05:00-08:00</updated>
  <author>
    <name>Violet Pe√±a</name>
  </author>
  <entry>
    <title>Private vs. Restricted Node Packages: What's the Diff?</title>
    <link rel="alternate" href="https://vgpena.github.io/private-vs-restricted-node-packages-what-s-the-diff/"/>
    <id>https://vgpena.github.io/private-vs-restricted-node-packages-what-s-the-diff/</id>
    <published>2018-12-28T15:05:00-08:00</published>
    <updated>2018-12-28T15:52:19-08:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">'Private' and 'Restricted' may sound similar, but they're completely different when it comes to Node packages. Here's what they mean and how to use them.</summary>
    <content type="html">&lt;p&gt;At &lt;a href="http://www.instrument.com/"&gt;work&lt;/a&gt;, my team is sharing React components with our client by packaging and publishing each component individually. Publishing Node packages is new territory for me, and the learning curve hasn‚Äôt always been gentle. I‚Äôll take some time now to clarify a paradigm I initially found confusing: what‚Äôs the difference between making a Node package &lt;em&gt;private&lt;/em&gt; and &lt;em&gt;restricting access&lt;/em&gt; to it?&lt;/p&gt;
&lt;h2 class='section-title' id=private-node-packages&gt;&lt;a href='#private-node-packages' class='section-inner'&gt;Private Node packages&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A private package is not shared or distributed on its own, and cannot be published to a registry (such as &lt;a href="https://www.npmjs.com/"&gt;npmjs.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-secondary"&gt;
    &lt;img src="/images/harriet.gif" alt="" title="Harriet the Spy shows us the cover of her journal, which reads PRIVATE." width="400px" height="232px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Marking a package as private ensures that its code will not be published, even accidentally. For instance, a one-off marketing site using React will contain a &lt;code&gt;package.json&lt;/code&gt; file. Unless this file specifies that the project is private, an errant contributor could publish the project code to a package registry, even if the code were proprietary or simply wouldn‚Äôt make sense to distribute on its own.&lt;/p&gt;

&lt;p&gt;Here‚Äôs a sample &lt;code&gt;package.json&lt;/code&gt; for a non-distributable project:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code json"&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"specific-website"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"version"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"0.0.1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"description"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"A bespoke website for &amp;lt;Client&amp;gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"main"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"index.js"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"private"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"scripts"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"dependencies"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"devDependencies"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Privacy only affects the ability to publish to a package registry ‚Äî it won‚Äôt affect other ways of sharing code, such as pushing to a Github repo.&lt;/p&gt;

&lt;p&gt;As a sidenote, private packages are generally ignored by packaging utilities. For example, &lt;a href="https://github.com/lerna/lerna"&gt;Lerna&lt;/a&gt; won‚Äôt include private packages in its &lt;code&gt;lerna list&lt;/code&gt; output unless you pass in the &lt;code&gt;--all&lt;/code&gt; flag.&lt;/p&gt;
&lt;h2 class='section-title' id=restricted-node-packages&gt;&lt;a href='#restricted-node-packages' class='section-inner'&gt;Restricted Node packages&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In contrast to private packages, restricted packages &lt;em&gt;can&lt;/em&gt; be distributed via a package registry, but only to whitelisted individuals. To use a restricted package, you need an account with the specific registry to which it was published, and you must belong to the organization or team that has access to that package. Otherwise, the package‚Äôs URL will 404.&lt;/p&gt;

&lt;p&gt;One use for restricted packages is for sharing proprietary code within a company. For example, a custom lazyloader that conforms to studio standards can be used on multiple projects either through copy-pasting the original code over and over (boo), or through publishing the code once to npmjs.com and including it as a dependency on subsequent projects (yay!).&lt;/p&gt;

&lt;p&gt;Here‚Äôs a sample &lt;code&gt;package.json&lt;/code&gt; for a project with locked-down distribution permissions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code json"&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"@company/awesome-lazyloader"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"version"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"0.0.1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"description"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"A reusable lazyloader for use at &amp;lt;Company&amp;gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"main"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"index.js"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"license"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"UNLICENSED"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"publishConfig"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
        &lt;/span&gt;&lt;span class="s2"&gt;"access"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"restricted"&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="s2"&gt;"dependencies"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;&lt;span class="w"&gt;  
      &lt;/span&gt;&lt;span class="s2"&gt;"devDependencies"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you publish to npmjs.com, take note that the site UI diverges from actual Node packaging terminology: a restricted package‚Äôs UI will have a ‚ÄúPrivate‚Äù badge even though the package &lt;em&gt;isn‚Äôt&lt;/em&gt; private. (I can‚Äôt explain this; it‚Äôs just bad UX.)&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/npmjs-com-private.png" alt="" title="A screengrab of a restricted package on npmjs.com; the site UI includes a &amp;quot;Private&amp;quot; badge under the package name." width="2470px" height="966px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Licensing&lt;/h3&gt;
&lt;p&gt;If you are publishing a restricted package, you probably want it to be unlicensed, which means that no one is inherently authorized to modify or redistribute it. You can set this either by removing the &lt;code&gt;license&lt;/code&gt; field from the project‚Äôs &lt;code&gt;package.json&lt;/code&gt; or by setting it to &lt;code&gt;license: &amp;quot;UNLICENSED&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 class='section-title' id=&gt;&lt;a href='#' class='section-inner'&gt;üëãüèª&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Thanks for reading! I hope this saved you some time. May 2019 be full of writing and sharing great JavaScript. ü•Çü•Ç&lt;/p&gt;

&lt;p&gt;Many thanks to Daniel, Ginger, and Thomas for your time, energy, and expertise.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Github Templates in Ten Minutes</title>
    <link rel="alternate" href="https://vgpena.github.io/github-templates/"/>
    <id>https://vgpena.github.io/github-templates/</id>
    <published>2018-08-27T07:55:00-07:00</published>
    <updated>2018-12-28T15:54:36-08:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">Github templates are an amazing project management tool. Pull Request and Issue templates help your team communicate and boost documentation. They're easy to write and completely customizable. Here's how to get started with Github templates and lead the charge for top-notch communication on your project team.</summary>
    <content type="html">&lt;p&gt;The better your team communicates, the more time you can spend on the fun stuff -- building interesting things and solving hard problems. When you&amp;#39;re looking for ways to improve communication, take a look at Github. They‚Äôve rolled out two features that should be incorporated into any project: Pull Request templates and Issue templates. In this post, I‚Äôll walk through these tools and how to leverage them on your next (or current) project.&lt;/p&gt;

&lt;p&gt;For sample code, and a cloneable project template (hint hint), see my &lt;a href="https://github.com/vgpena/gh-docs-boilerplate"&gt;gh-docs-boilerplate repo&lt;/a&gt;. üíÖüèª&lt;/p&gt;
&lt;h2 class='section-title' id=pull-request-templates&gt;&lt;a href='#pull-request-templates' class='section-inner'&gt;Pull Request Templates&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We‚Äôve all been there: a pull request rolls in, titled ‚ÄúUpdates‚Äù, with no description and 22 files changed.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-primary"&gt;
    &lt;img src="/images/github-templates/bad-pr.png" alt="" title="A PR with very little information or context." width="1592px" height="468px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;üëçüèª LGTM&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;We‚Äôre not sure what it does, which tickets it closes, or how to test it. Until the person who opened the PR is given a chance to explain themself, project managers and fellow engineers can‚Äôt track on what work has been done. Over time, this lack of context and consistency slows work down and leads to misunderstandings.&lt;/p&gt;

&lt;p&gt;Enter Pull Request templates. These are simple Markdown files that prepopulate pull request description fields. You can put this template in your repository root, a &lt;code&gt;/docs&lt;/code&gt; folded, or a &lt;code&gt;/.github&lt;/code&gt; folder. Once it exists on your default branch (usually &lt;code&gt;master&lt;/code&gt; or &lt;code&gt;dev&lt;/code&gt;), its content will flow into all newly opened PRs.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-primary"&gt;
    &lt;img src="/images/github-templates/good-pr.png" alt="" title="When you create a PR template, its contents populate new PRs." width="1578px" height="1410px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;One click and you&amp;#39;re halfway there.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/vgpena/gh-docs-boilerplate/blob/master/.github/pull_request_template.md"&gt;Here‚Äôs my starter template&lt;/a&gt;; I like to list out headings for any information I want from a contributor. You almost certainly want these questions answered:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What changes does this branch contain?&lt;/li&gt;
&lt;li&gt;What issues/tickets does this branch close?&lt;/li&gt;
&lt;li&gt;How can I test this code?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on your project and workflow, you may want to include places for contributors to upload screenshots, link to prototypes, or (my favorite) embed GIFs that reflect their feelings towards that PR.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-primary"&gt;
    &lt;img src="/images/github-templates/mainframe.gif" alt="" title="A glitch art + muppets GIF." width="400px" height="225px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Sample GIF included for your inspiration.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 class='section-title' id=issue-templates&gt;&lt;a href='#issue-templates' class='section-inner'&gt;Issue Templates&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Communication and documentation can also be skimped on when opening issues. We‚Äôve all been guilty of creating issues with vague titles and no context.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-primary"&gt;
    &lt;img src="/images/github-templates/bad-issue.png" alt="" title="" width="1592px" height="468px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Time for some detective work!&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;If I didn‚Äôt personally open this ticket, I don‚Äôt know what needs to be done or what success looks like. I can‚Äôt prioritize it or estimate its complexity.&lt;/p&gt;

&lt;p&gt;Issue templates, like PR templates, are Markdown files that provide default content for Issues opened via Github. They go in the &lt;code&gt;/.github/ISSUE_TEMPLATE&lt;/code&gt; folder. Like PR templates, they only kick in when on your default branch.&lt;/p&gt;

&lt;p&gt;Issue templates go one step farther than PR templates by letting you create multiple issue types, which are all available in a choose-your-own-adventure issue picker.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/github-templates/issue-picker.png" alt="" title="Define multiple Issue templates, and you get a cool Issue picker on Github." width="1586px" height="580px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Selecting an issue type will flow that specific template into your new issue.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/github-templates/good-issue.png" alt="" title="Once you pick an issue template, its text prepopulates a new issue." width="1558px" height="1152px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;You can define as many issue formats as you want and tailor them to your project. For example, a recent project of mine had five total template types. The Backend template had a prompt to list out any tests that should be written; the Frontend template asked for embedded design comps.&lt;/p&gt;

&lt;p&gt;I recommend having at least three issue templates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;New Feature&lt;/li&gt;
&lt;li&gt;Change Request&lt;/li&gt;
&lt;li&gt;Bug Report&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These buckets encompass almost all changes made to a project. They help team members include the right information (no more bug reports without browser/OS), and they also help define the task in the first place. Knowing, and making explicit, whether something is a bugfix or an update, a change or a new feature, helps clarify its implications and priority.&lt;/p&gt;

&lt;p&gt;I have three issue templates set up &lt;a href="https://github.com/vgpena/gh-docs-boilerplate/tree/master/.github/ISSUE_TEMPLATE"&gt;here&lt;/a&gt;. I recommend writing your own set of templates to reflect your ticketing flow and project needs.&lt;/p&gt;
&lt;h2 class='section-title' id=in-the-wild&gt;&lt;a href='#in-the-wild' class='section-inner'&gt;In the Wild&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Congrats, you added PR and Issue templates to your project! üéâ As your project evolves, revisit them every so often to make sure they still meet your needs. It only takes a few minutes to add and remove fields, swap out or edit templates. Give any changes about a week to sink in; if they still don‚Äôt feel right, group up with your team and figure out something better. It‚Äôll be a quick meeting, and you‚Äôll get to learn about what each other do and need.&lt;/p&gt;
&lt;h2 class='section-title' id=but-wait-theres-more&gt;&lt;a href='#but-wait-theres-more' class='section-inner'&gt;But Wait, There‚Äôs More!&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Github offers other low-effort ways to improve communication:&lt;/p&gt;
&lt;h3 tabIndex=0&gt;The Readme&lt;/h3&gt;
&lt;p&gt;This is a low bar to clear, but I‚Äôm regularly shocked by how many projects don‚Äôt have a proper Readme. A brief write-up on what the project does and how to install it is invaluable. Even if your project is a fork or clone of &lt;em&gt;another&lt;/em&gt; project, update your Readme ‚Äî surely something about &lt;em&gt;your&lt;/em&gt; project is different from the source material.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Code of Conduct&lt;/h3&gt;
&lt;p&gt;Adding a &lt;a href="https://help.github.com/articles/adding-a-code-of-conduct-to-your-project/"&gt;Code of Conduct&lt;/a&gt; to your project is easy, and it‚Äôs a powerful way to manage community on open-source projects. If you add one to your repo, it will be linked to in the issue creation flow.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Contribution Guidelines&lt;/h3&gt;
&lt;p&gt;Like a CoC, &lt;a href="https://help.github.com/articles/setting-guidelines-for-repository-contributors/"&gt;Contribution Guidelines&lt;/a&gt; clarify community standards and keep discussion productive. This will also be linked to when issues are opened. Contribution guidelines and CoCs are most applicable to open-source software, but it‚Äôs never &lt;em&gt;bad&lt;/em&gt; to codify expectations, even within a closed team.&lt;/p&gt;
&lt;h2 class='section-title' id=tldr&gt;&lt;a href='#tldr' class='section-inner'&gt;TL;DR&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Better communication = more time building fun things. Github can help you. Use &lt;a href="https://github.com/vgpena/gh-docs-boilerplate"&gt;my boilerplate&lt;/a&gt; to make Pull Request and Issue templates, sit back, and enjoy the clarity üòé&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Hard Reset</title>
    <link rel="alternate" href="https://vgpena.github.io/hard-reset/"/>
    <id>https://vgpena.github.io/hard-reset/</id>
    <published>2018-07-04T14:10:00-07:00</published>
    <updated>2018-07-04T14:32:33-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">Getting away, physically and mentally, is something that I need in order to stay healthy. Of course I only realize this when it's too late. Here's an ode to nature, thoughts on burnout, and a plan to stay happy.</summary>
    <content type="html">&lt;p&gt;Two nights ago, I slept in my own bed for the first time in almost three weeks.&lt;/p&gt;

&lt;p&gt;My boyfriend and I had just returned from a two-and-a-half-week-long voyage through Amsterdam, Dubrovnik, and London. We‚Äôd seen &lt;em&gt;Sunflowers&lt;/em&gt; and the Rosetta Stone; we‚Äôd kayaked and jetskied; we‚Äôd eaten slabs of tongue-burning cheese pastry and spooky but delectable foamed asparagus.&lt;/p&gt;

&lt;p&gt;But more valuable than these experiences, than fun, than knowledge, is simply having my head screwed on straight again.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I moved to Portland in the fall of 2014 after finding that, as much as I wished otherwise, I was fundamentally incompatible with New York City. When people (usually my Detroit-based relatives; usually at Passover) ask me what I like about living here, I find myself invoking the usual Portland clich√©s: it‚Äôs highly bikeable; the food is top-notch; it‚Äôs close to nature.&lt;/p&gt;

&lt;p&gt;This last point has done me incalculable good over the three and a half years that I‚Äôve dwelled here. Too much good to dissect quickly with family over a noisy dinner; too much for even me to understand without careful consideration.&lt;/p&gt;

&lt;p&gt;So here we go.&lt;/p&gt;

&lt;p&gt;I‚Äôd basically never hiked before moving out here. Growing up in Buffalo, NY, I‚Äôd been on a handful of creek walks; I knew how to canoe, start a fire, identify nascent hypothermia. But not seeing another person for hours, for a day; anticipating and responding to hunger, thirst, cold, tiredness; drawing together two points on a map using only my own body, using my tiny self to carve new spaces out of the soil, out of air? Not a chance. The monotony, wonder, calculus, poetics of hiking were new to me, alien experiences adjacent to my everyday life. Stone, trees, mulch are there even now, separated by a spun-sugar membrane from my home and bed where I write this.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;figure role="img" class=" image-primary"&gt;
    &lt;img src="/images/cascades.JPG" alt="A photo of a lake in a valley ringed by snowy mountains." title="The view from a snowy ridge in the North Cascades." width="1360px" height="1020px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;North Cascades, Washington State, Fall 2017. I camped by this lake.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;When you are hiking, you are thinking about nothing else. Even if you‚Äôre talking about work, about friends, about books with your hiking companions, there is always part of your mind that‚Äôs focused not on that conversation but on sensation, on your surroundings. Part of you is aware only of smells, of temperature, of your own breathing. And when silence falls, when the conversation dies, that part of your mind expands to fit the space available.&lt;/p&gt;

&lt;p&gt;For infinite moments I have been a valley, been distance, granite, innumerable trees.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;This time spent outside of myself is stranger, more elusive, than as to be easily described. I sleep well that night; I go to work the next week and spend a few minutes confused: what am I doing here; what was I working on on Friday?&lt;/p&gt;

&lt;p&gt;But once the confusion clears, I feel amazing. The top sheet of paper has been torn from a notepad, revealing a completely blank page underneath. I have never been tired; I have never been hurt. How do I feel this okay?&lt;/p&gt;

&lt;p&gt;As I realized that this was a pattern, a name presented itself: a hard reset.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;A hard reset is when you bypass normal, software-based ways of shutting down or restarting something, usually a computer. Instead of telling the machine to shut itself down via a dropdown option, waiting for programs to close, and telling it what to do with unsaved documents and so on, you simply hold down the Power button until the screen goes dark. When your computer boots up again it has little or no idea what it was just doing. It might have a recovery state saved that you could invoke to reopen some programs, but by and large the machine is in a fresh, &lt;em&gt;who am I and what year is it&lt;/em&gt; state.&lt;/p&gt;

&lt;p&gt;This is how I am the day or two after coming back from a long hike. Being forced to (or getting to) think about hiking and only hiking for an entire day shifts my mind into a different track. I slow down, or at least my thoughts cluster differently. I don‚Äôt, I can‚Äôt think about a meeting or an architecture decision, because I‚Äôm noticing how the ground feels, listening for the first whispers of thirst, noticing acutely and dimly the ferns that litter my path.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-secondary"&gt;
    &lt;img src="/images/mthood.jpg" alt="A close-in photo of a snow-covered mountain." title="Looking up to Mount Hood." width="1360px" height="1020px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Mt. Hood from Cairn Basin, Summer 2016.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;This hard reset has become supremely important for me; this is the first time I‚Äôve been able to identify that I need it &lt;em&gt;and&lt;/em&gt; been able to get it. For me, the cart came before the horse in that I got it and only later realized how okay it made me feel.&lt;/p&gt;

&lt;p&gt;(In my experience, hard resets make me feel wonderful, happy, good, etc. but above all &lt;em&gt;okay&lt;/em&gt; in the positive sense ‚Äî buoyant, stable, and whole. As I mention above: untired; unhurt.)&lt;/p&gt;

&lt;p&gt;When I lived in New York City, life maintained a death grip on me. No matter what I did, I was me doing nothing but living my life, my step-by-step life in a body in a place. And it added up.&lt;/p&gt;

&lt;p&gt;I didn‚Äôt move to Portland for the hard reset or for work-life balance. I didn‚Äôt know about hard resets, and balance is a nut I have yet to crack (but I‚Äôm trying). I moved here for a job, because I always move for jobs. I took the job that paid the most, the single job I applied for outside of the NYC area, the escape hatch I traced for myself out of what I slightly already understood was an untenable life.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;This summer, I needed something deeper than a hard reset. I love work and I still love my job, but the downside of this is that it has a way of creeping into hours and cycles that I should be spending doing anything but working. I may not be in the office but as I play games, read, garden, some part of my brain is mulling over how to order upcoming features, onboard a new team member, handle a change to the stack.&lt;/p&gt;

&lt;p&gt;To be clear, I let this happen. Compartmentalization is a skill, one that I learned from my violin teacher. As he reworked my bow hold and gave me intervals to practice, he also helped train me to leave thoughts at the door, to make room for only what I needed in the moment.&lt;/p&gt;

&lt;p&gt;(He also noticed that, when playing, my breath sometimes became irregular: I was forgetting to breathe.)&lt;/p&gt;

&lt;p&gt;I let those skills slide, let my hard resets slide in the name of &lt;em&gt;career advancement&lt;/em&gt; and out of genuine pleasure at the challenges I had been handed. It was fun until it wasn‚Äôt. Until I was coming in on Mondays still-tired and still-hurt and trying everything to make it not show. I would come home late, ground down, unable to stop thinking about that day and about the one coming up.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;It wasn‚Äôt burnout, or maybe it was? Does it count as burnout if you never cry &lt;em&gt;at&lt;/em&gt; work? If you never yell at anyone, if you never look for another job?&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I put an out-of-office responder on my email and could finally exhale. We were halfway through a Van Gough exhibit in Amsterdam. We paused between floors and I hit some buttons to give myself the space that an ocean hadn‚Äôt been able to supply.&lt;/p&gt;

&lt;p&gt;I thought I would be better by the time we got to Croatia. I had allocated a few days to feel tired, to feel okay-but-down in that way you need to allow yourself to feel sometimes, because that‚Äôs the only way to not make it worse.&lt;/p&gt;

&lt;p&gt;I was not better in Croatia. Thinking about work made my chest tighten. My boyfriend pecked merrily away at a side project; opening a text editor made me want to puke. I read a book, gazed at the sea, played games, was not myself for a bit. Some days I felt solidly better than before; one day I woke up and was sad from that moment until I went to sleep that night. I still don‚Äôt know why.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class=" image-secondary"&gt;
    &lt;img src="/images/dubrovnik.jpg" alt="A photo of an old city with narrow streets and orange tiled roofs; the sea is visible beyond." title="The old section of Dubrovnik." width="1360px" height="1020px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Dubrovnik from the old city wall, Summer 2018. Getting better.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;I was better on my third day in London. I woke up early and walked around alone. Dipped in and out of shops; felt the undulations of building patterns across streets, neighborhoods. Bought a pen and was happy. I felt autonomous; I felt myself wanting things.&lt;/p&gt;

&lt;p&gt;That afternoon, I wrote some code. Because I wanted to. I slept well.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Here I am, home again, working on finding and maintaining this peace. What do I need to be happy? How do I stay buoyant?&lt;/p&gt;

&lt;p&gt;I thought I was low-maintenance but I guess not, or maybe this &lt;em&gt;is&lt;/em&gt; low-maintenance, or maybe the metric of ‚Äúwhat do you need to do in your free time to be happy at work‚Äù is backwards and inside-out. (Kidding; I &lt;em&gt;know&lt;/em&gt; it is.)&lt;/p&gt;

&lt;p&gt;This is my journey to consistent wellness; I assume it will take forever, but maybe it will get easier someday.&lt;/p&gt;

&lt;p&gt;In the meantime, here I will be, giving myself (making for myself) the space and the circumstances to be well.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Good is better than perfect</title>
    <link rel="alternate" href="https://vgpena.github.io/good-is-better-than-perfect/"/>
    <id>https://vgpena.github.io/good-is-better-than-perfect/</id>
    <published>2017-12-27T11:42:00-08:00</published>
    <updated>2018-07-03T11:21:36-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">Creation can be really hard because of the standards we set for ourselves. Here's how I stop worrying about perfection so that I can actually get things done.</summary>
    <content type="html">&lt;p&gt;I don‚Äôt like my blog.&lt;/p&gt;

&lt;p&gt;To clarify: as of writing, I‚Äôm lukewarm on how article titles look. I don‚Äôt have per-post tags; I haven‚Äôt added pagination to the index. I could go on.&lt;/p&gt;

&lt;p&gt;When I launched, about ten months ago, my list of complaints was even longer. I didn‚Äôt love my body font; I &lt;em&gt;really&lt;/em&gt; didn‚Äôt love my &lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt;s. I hadn‚Äôt written styles for section headers, tables, or asides.&lt;/p&gt;

&lt;p&gt;But I had made something.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;This is actually the third iteration of my blog; the first two lived only on my local server, each &lt;code&gt;rm -r&lt;/code&gt;&amp;#39;d when I realized that something about them wasn‚Äôt, and could never be, perfect.&lt;/p&gt;

&lt;p&gt;As I laid the groundwork for my third version, I promised myself that &lt;em&gt;this&lt;/em&gt;, no matter what, would be what I launched. I was done spinning my wheels on mechanical problems; I needed to move past that and start actually writing.&lt;/p&gt;

&lt;p&gt;Because, naturally, my quest for perfection wasn‚Äôt just because I wanted to make a good-looking site. It was also a bid to avoid ever writing a single post. After all, if my site wasn&amp;#39;t done, how could I publish things on it?&lt;/p&gt;

&lt;p&gt;It‚Äôs so much easier to write CSS for myself than it is to write English for other people.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;But here I am, ten months later, with a still-flawed &lt;em&gt;but productive&lt;/em&gt; site, a site which lets me express myself, spread knowledge, and hone my writing skills. It&amp;#39;s not perfect. But, perfections included, it&amp;#39;s exactly right for me.&lt;/p&gt;
&lt;h2 class='section-title' id=how-to-create-something-imperfect&gt;&lt;a href='#how-to-create-something-imperfect' class='section-inner'&gt;How to create something imperfect&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Most people I&amp;#39;ve talked to who self-host a blog get bogged down, as I did, in the creation step.&lt;/strong&gt; This is a reliable pattern because webdev is hard &lt;em&gt;and writing is even harder&lt;/em&gt;. This is fine. You&amp;#39;re fine. Make something good, make something that supports what you immediately need it to support, and get it out. the. door.&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a few things that helped me:&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Create content, then develop to that spec&lt;/h3&gt;
&lt;p&gt;Since you can&amp;#39;t control another separate human&amp;#39;s actions, client work usually involves some kind of attempt &lt;a href="https://en.wikipedia.org/wiki/Three-body_problem"&gt;to anticipate every single possible thing they might want to do&lt;/a&gt;, which is a) super obnoxious and b) super fun.&lt;/p&gt;

&lt;p&gt;How should we crop and position images if the client tries to upload something 800px by 80px? How should we space out the title if they only make it one word long? What if they create an article and then leave the body completely blank? Et cetera.&lt;/p&gt;

&lt;p&gt;When you&amp;#39;re your own client, you don&amp;#39;t have to build what you don&amp;#39;t need.&lt;/p&gt;

&lt;p&gt;In my case, I drafted a couple of articles so that I wouldn&amp;#39;t deploy an empty blog (SO EMBARASSING RITE???). As soon as they were vaguely done, I switched over to use those, not Lorem Ipsum, as dummy content. I was able to build to my own exact needs, so I made a blog that worked perfectly &lt;em&gt;even though&lt;/em&gt; I hadn&amp;#39;t written styles for tables or even section headings.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m not advising to make bespoke one-offs, or to work without considering stability/extensibility. Make a system, but accept that that system won&amp;#39;t be all-encompassing. Make a system that you intend to improve. Make the system &lt;em&gt;that you need&lt;/em&gt;.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Remember that you don&amp;#39;t work in print&lt;/h3&gt;
&lt;p&gt;Logically, this isn&amp;#39;t difficult, but something about it is emotionally hard for me.&lt;/p&gt;

&lt;p&gt;If you think your site is ugly but you push ahead and publish a couple of posts, you can deploy better styles later and everything will now be pretty, &lt;em&gt;even the posts that were originally ugly&lt;/em&gt;. Hooray for the internet!&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/first-try.gif" alt="" title="Animation from The Lego Movie: Batman throws Batarangs at a button until he hits it." width="358px" height="231px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;IT&amp;#39;LL BE FINE.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Know that you&amp;#39;re not famous&lt;/h3&gt;
&lt;p&gt;(I assume.)&lt;/p&gt;

&lt;p&gt;Anyway, a lot is made of splashy launches and Sticking The Landing, but whoever your idol is &lt;em&gt;is definitely not reading your blog right now&lt;/em&gt;. You have plenty of time to find your voice, fix your wonky line-heights, etc. before it&amp;#39;s read by Oprah or Dan Abramov or whoever. This time is a rehearsal, and it&amp;#39;s valuable.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Don&amp;#39;t mistake labor for progress&lt;/h3&gt;
&lt;p&gt;Is your goal to make a good website, or is it to become a better writer?&lt;/p&gt;

&lt;p&gt;I don&amp;#39;t know how long it takes to get comfortable writing, to find your voice, to carve out a good schedule, to pick  topics that you reliably want to write about. But my blog has been live for ten months and I can tell you that I&amp;#39;m not there yet.&lt;/p&gt;

&lt;p&gt;So let&amp;#39;s say that it takes two or three years of low-intensity blogging before you develop proficiency.&lt;/p&gt;

&lt;p&gt;That timer doesn&amp;#39;t start until you&amp;#39;re actually publishing posts. You can toil away for a year and a half on an amazing perfect üëåüèªüî•üíØ website; this will have zero effect on your writing prowess.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make something that advances you towards your goal&lt;/strong&gt;, or at least be honest about what you&amp;#39;re working on and whether it&amp;#39;s part of that specific journey. I&amp;#39;ve taken detours and they&amp;#39;ve been helpful and lovely but I&amp;#39;m not going to pretend that revamping my font-loading made me a better writer.&lt;/p&gt;
&lt;h2 class='section-title' id=the-journey&gt;&lt;a href='#the-journey' class='section-inner'&gt;The journey&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Over the past ten months, I&amp;#39;ve held tight to these maxims -- they&amp;#39;ve been invaluable not only while creating the blog, but since then, as I continue to write and publish.&lt;/p&gt;

&lt;p&gt;If you&amp;#39;ve been looking for a sign to start a blog or launch a project, let this be it. You&amp;#39;ll love your imperfect, real creations a thousand times more than something flawless that&amp;#39;s never seen the light of day.&lt;/p&gt;

&lt;p&gt;Excited for what you make in 2018! üíÉüèª&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Finding JavaScript bliss with the Chrome debugger</title>
    <link rel="alternate" href="https://vgpena.github.io/debugging-bliss/"/>
    <id>https://vgpena.github.io/debugging-bliss/</id>
    <published>2017-09-27T16:01:00-07:00</published>
    <updated>2018-07-03T11:21:36-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">Google Chrome's built-in debugger is the best option out there for writing, debugging, and understanding JavaScript. Here's how to get started using it today.</summary>
    <content type="html">&lt;p&gt;I&amp;#39;m always on the lookout for ways to make my job easier. How can I build out functionality more quickly? How can I understand my code better? How can I debug more reliably?&lt;/p&gt;

&lt;p&gt;The best tool for doing all of this has been &lt;strong&gt;Google Chrome&amp;#39;s built-in debugger&lt;/strong&gt;. Yep, I was surprised, too -- I didn&amp;#39;t even realize it existed for years. Since then, it&amp;#39;s been my go-to aide for everything JavaScript.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/debugging/sombra.jpg" alt="" title="Sombra from Overwatch doing hacker things." width="1600px" height="1066px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Actual photo of someone using the Chrome debugger.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Learning how to use the Chrome debugger helps me write and debug more quickly, but more than just that, I actually &lt;em&gt;understand&lt;/em&gt; code better. I can isolate behavior, test out ideas, and feel my way around the codebase more quickly and reliably than ever before.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s high time to pay this forward, so here&amp;#39;s a primer on the Chrome debugger. Hope it&amp;#39;ll make your lives easier! üíñ&lt;/p&gt;
&lt;h2 class='section-title' id=the-debugging-landscape&gt;&lt;a href='#the-debugging-landscape' class='section-inner'&gt;The debugging landscape&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;So if I just found about the debugger, how have I been debugging code until now?&lt;/p&gt;

&lt;p&gt;There are three major options for debugging frontend JavaScript. I&amp;#39;ll run over what they are and why two of them fall short. üòà&lt;/p&gt;

&lt;p&gt;PS, you can 100% use the Chrome debugger on backend JS as well -- check out the very end of this post. üòàüòà&lt;/p&gt;

&lt;p&gt;PPS, Safari &lt;em&gt;also&lt;/em&gt; has a built-in debugger with a really similar feature set. I don&amp;#39;t use Safari much, though, so I&amp;#39;ll let someone else write that blog post üòàüòàüòà&lt;/p&gt;
&lt;h3 tabIndex=0&gt;alert(): not even once&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;alert(value);&lt;/code&gt; pops up a browser-level modal containing &lt;code&gt;value&lt;/code&gt;. It‚Äôs often the first debugging method taught to JS newbies; it‚Äôs concise, easy to understand, and provides immediate feedback. Unfortunately, &lt;strong&gt;it‚Äôs a terrible debugging utility and you shouldn‚Äôt use it&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-secondary"&gt;
    &lt;img src="/images/debugging/alert.gif" alt="" title="An alert popup." width="1134px" height="652px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code javascript"&gt;&lt;code&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;foo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'bar'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nx"&gt;alert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here are some reasons to stop using &lt;code&gt;alert()&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;It doesn‚Äôt support our debugging needs&lt;/strong&gt;. &lt;code&gt;alert()&lt;/code&gt; isn‚Äôt content-agnostic and it can only output certain data types. For example, if you try to print an &lt;code&gt;Object&lt;/code&gt;, the alert will show the monumentally unhelpful &lt;code&gt;object Object&lt;/code&gt;. You can print out the actual contents of the Object by converting it to a String first, but this is a clear indication that &lt;code&gt;alerts&lt;/code&gt; aren‚Äôt prepared to offer what we need from them.
&lt;figure role="img" class=" image-primary"&gt;
&lt;img src="/images/debugging/object-object.gif" alt="" title="An alert popup being unhelpful." width="1178px" height="552px" /&gt;
&lt;figcaption&gt;
&lt;p&gt;IT BURNS US PRECIOUS&lt;/p&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;It doesn‚Äôt include contextual information.&lt;/strong&gt; &lt;code&gt;alert&lt;/code&gt;s don‚Äôt by default include the line number or file name of where that &lt;code&gt;alert&lt;/code&gt;  was created. This puts the onus on &lt;em&gt;you&lt;/em&gt; to manually include identifying information in your alert messages if you‚Äôre using &lt;code&gt;alert&lt;/code&gt; in multiple places.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Any alerts left in production code will create visible, disruptive popups for all users.&lt;/strong&gt; Enough said.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Alerts can cause unintended side effects.&lt;/strong&gt; Alerts are thread-blocking, so nothing else will happen while one is open. Your page will halt loading; animations will pause (but may jump ahead upon closing an alert, depending on how they&amp;#39;re implemented). You need service workers to get around this threading issue, and those service workers could be used to solve much cooler problems.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 tabIndex=0&gt;console.log(): you could do better&lt;/h3&gt;
&lt;p&gt;You also have the much more helpful (but not amazing! keep reading) option of printing information to the console available in Chrome‚Äôs DevTools using &lt;code&gt;console.log(value)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Console fills in a lot of the gaps that &lt;code&gt;alert&lt;/code&gt; lacks, such as line numbers and filenames. It&amp;#39;s also much more robust: you can log Objects directly without having to coerce them into Strings, and you can drill down into Objects and Arrays to inspect their contents. You can even get fancy and indicate importance with different log levels, such as &lt;code&gt;console.warn()&lt;/code&gt; and &lt;code&gt;console.error()&lt;/code&gt; üíÖ&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-secondary"&gt;
    &lt;img src="/images/debugging/console.gif" alt="" title="The console outputting a message." width="1134px" height="652px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code javascript"&gt;&lt;code&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;foo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'bar'&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nx"&gt;console&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;foo&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Console messages should also be removed in production, but at least they will only be visible to visitors who have their consoles open; i.e., other nerds. Also unlike alerts, &lt;em&gt;they will completely break your site for IE9 users&lt;/em&gt;. &lt;a href="https://stackoverflow.com/a/7742862"&gt;IE9 and below have a console that only exists while DevTools is open&lt;/a&gt;, so any errant &lt;code&gt;console&lt;/code&gt;s left in production will nuke those users‚Äô JS.&lt;/p&gt;

&lt;p&gt;Console is way more helpful than &lt;code&gt;alert&lt;/code&gt;, but it still falls short in complex situations. There are abundant StackOverflow questions, &lt;a href="https://stackoverflow.com/questions/30150469/why-console-log-displays-incorrect-objects-values"&gt;such as this one&lt;/a&gt;, stemming from difficulties getting Console to accurately report changing values. Similar to how &lt;code&gt;[object Object]&lt;/code&gt; shows that &lt;code&gt;alert()&lt;/code&gt; isn&amp;#39;t built to address certain needs, this issue with the console suggests that &lt;code&gt;console&lt;/code&gt; isn&amp;#39;t enough for us, either.&lt;/p&gt;

&lt;p&gt;Console can also get confusing if you‚Äôre trying to dig into a large object or set of objects due to the sheer amount of output that you receive. It can be overwhelming to use console to monitor an animation on &lt;code&gt;requestAnimationFrame&lt;/code&gt; or track down a value change in a mobx store.&lt;/p&gt;

&lt;p&gt;Console is an essential tool for frontend developers, and it‚Äôs a huge step up from Alert, but it‚Äôs not the answer to everything. To supercharge your debugging powers, let‚Äôs turn to‚Ä¶&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Interactive debugging: happily ever after&lt;/h3&gt;
&lt;p&gt;Interactive debugging using Chrome&amp;#39;s debugger is a vastly different tool than Console or Alert. While the latter two are limited to printing out specific values, interactive debugging lets you roam around your code while accessing all the values in scope at that point in time.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/debugging/neo.webp" alt="" title="Neo dodges bullets, like you dodge unneeded console messages." width="490" height="275px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Another actual photo of someone using the Chrome debugger.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;You don&amp;#39;t even have to write any code for this one. All you need is to open Chrome&amp;#39;s DevTools via &lt;code&gt;Cmd+Option+I&lt;/code&gt; or &lt;code&gt;View &amp;gt; Developer &amp;gt; Developer Tools&lt;/code&gt; and then click on the Sources tab.&lt;/p&gt;

&lt;p&gt;Okay, ready?&lt;/p&gt;
&lt;h2 class='section-title' id=let39s-get-debugging&gt;&lt;a href='#let39s-get-debugging' class='section-inner'&gt;Let&amp;#39;s get debugging&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Chrome‚Äôs interactive debugging tools work on any JavaScript, but they‚Äôre most helpful if your code has not been &lt;strong&gt;uglified, minified, concatenated, or obfuscated&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;These are project-by-project (maybe even file-by-file) configurations. You may have to make development vs. production webpack configs; you may already be good to go.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Meet the Sources tab&lt;/h3&gt;
&lt;p&gt;All your debugging tools live in Chrome‚Äôs Sources tab. Open this via the menu options &lt;code&gt;View &amp;gt; Developer &amp;gt; Developer Tools&lt;/code&gt; or pressing &lt;code&gt;Cmd+Option+I&lt;/code&gt; and then clicking to view the Sources section. The panel that opens should look something like this:&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/debugging/sources.png" alt="A screenshot of the Sources tab in Chrome DevTools. There are several panels full of different debug tools; the main panel is reserved for viewing code." title="The Sources tab of Chrome DevTools." width="1108px" height="652px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Yep, that&amp;#39;s kind of a lot at once. I don&amp;#39;t even use everything in here. But it&amp;#39;s really easy to use once you know what to look for.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Fuzzy finder activate!&lt;/h3&gt;
&lt;p&gt;While focused inside the Sources tab, you can open a specific file by pressing &lt;code&gt;Cmd + P&lt;/code&gt; to open the fuzzy finder. Start typing with this open to target a specific JS file that you want to poke around in.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/debugging/fuzzy-finder.gif" alt="" title="Chrome's fuzzy finder." width="990px" height="437px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Setting breakpoints&lt;/h3&gt;
&lt;p&gt;All your power in the debugger comes from setting up &lt;strong&gt;breakpoints&lt;/strong&gt; in your code. These are little flags that you drop down to pause execution of JavaScript.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You make a breakpoint&lt;/strong&gt; by clicking in the left margin next to a line of code; you can remove breakpoints in the same way. Notice that breakpoints appear in a master list as well as being marked in their home file.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/debugging/breakpoints.gif" alt="" title="Setting breakpoints in Chrome." width="990px" height="499px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;So if you drop a breakpoint in some code that executes when the document is ready and then refresh, that breakpoint gets hit, your code stops, and a scrim drops over your viewport.&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/debugging/hit-breakpoint.gif" alt="" title="What hitting a breakpoint looks like." width="2856px" height="1276px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;Doesn‚Äôt sound too exciting, but trust me, it gets pretty amazing‚Ä¶&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Inspecting values&lt;/h3&gt;
&lt;p&gt;While you‚Äôre halted at a breakpoint, your debugger has access to &lt;strong&gt;all variables currently in scope&lt;/strong&gt;. You can inspect these values in three ways:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;hover over that variable and a tooltip will show up:
&lt;div class=" image-primary"&gt;
&lt;img src="/images/debugging/tooltips.gif" alt="" title="Hovering to show tooltips" width="990px" height="499px" /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This is a super lightweight, immediate way to view variables on the fly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;type that variable name in your console and hit &lt;code&gt;Enter&lt;/code&gt;.
&lt;div class=" image-primary"&gt;
&lt;img src="/images/debugging/console-inspection.gif" alt="" title="Logging variable values" width="990px" height="870px" /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This will feel the most familiar for fans of &lt;code&gt;console.log()&lt;/code&gt;, but in this case you don&amp;#39;t have to wait for your code to recompile ü•Ç&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;find that variable in the &lt;code&gt;Scope&lt;/code&gt; pane of the Sources panel.
&lt;div class=" image-primary"&gt;
&lt;img src="/images/debugging/scope-inspect.gif" alt="" title="Using the Scope pane to inspect variable values." width="1594px" height="636px" /&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This pane can feel buried under the &lt;code&gt;Call Stack&lt;/code&gt; pane, but it&amp;#39;s the most comprehensive option of the three. It shows you all variables currently in scope, including global values that you might not even remember setting.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 tabIndex=0&gt;Moving around&lt;/h3&gt;
&lt;p&gt;You aren‚Äôt limited, however, to inspecting exactly around the breakpoints you set. The last major part of interactive debugging is this row of buttons:&lt;/p&gt;

&lt;p&gt;&lt;div class=" image-primary"&gt;
    &lt;img src="/images/debugging/buttons.png" alt="Buttons are: play/pause, step over, step in, step out, and mute all breakpoints." title="Your movement buttons." width="1108px" height="989px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;These buttons let you walk around across functions and files while keeping your inspector powers. Being able to do this is invaluable, but it can feel confusing or overwhelming, especially if you‚Äôre dealing with a lot of third-party code. Here‚Äôs what each of the buttons mean, and when you might want to use them:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Play/pause&lt;/strong&gt;: Start or stop script execution. This is how you start executing script again from inside a breakpoint.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step over:&lt;/strong&gt; Execute the current line of code and pause on the next line. This is great for when you need to track what‚Äôs happening inside of a specific function, or what happens to a value on a line-by-line basis.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step in:&lt;/strong&gt; Dive into the body of the current function call and pause on the first line of that function. This lets you inspect the inner workings of a function, as opposed to &lt;strong&gt;step over,&lt;/strong&gt; which will execute the entire function without pausing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step out:&lt;/strong&gt; Finish executing the current function body and pause on the line that called it. This is the opposite of &lt;strong&gt;step in&lt;/strong&gt; and is useful when you want to see where your current function was called from or you don‚Äôt want to have to &lt;strong&gt;step over&lt;/strong&gt; every line of the function in order to see where it is applied.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can use these movement options to track values across files, investigate individual values every time a function is called, etc. They‚Äôre incredibly useful and my best advice/non-advice is to play around with them until you develop a sense of how they behave and when to use them.&lt;/p&gt;

&lt;p&gt;Next to these buttons is a button to &lt;strong&gt;disable all breakpoints;&lt;/strong&gt; this button lets you ‚Äúmute‚Äù all current breakpoints without actually deleting (and potentially having to re-add) them.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Debugging best practices&lt;/h3&gt;
&lt;p&gt;Apart from being absurdly useful, one advantage of using the debugger as your sole logging/debugging tool is that all the stuff you‚Äôre adding &lt;strong&gt;only exists for you&lt;/strong&gt;. Debugging hooks shouldn&amp;#39;t be a part of production code and the Chrome debugger makes it very easy to make sure this happens.&lt;/p&gt;

&lt;p&gt;That said, you also have the option of inserting a breakpoint into your actual code. Put a line in your JS that says &lt;code&gt;debugger;&lt;/code&gt; and when the browser reaches that line, it‚Äôll pause and then you can poke around and inspect values as described.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Don‚Äôt do this.&lt;/strong&gt; I‚Äôm talking about it for completeness‚Äô sake, but it‚Äôs not a good idea. (Full disclosure: before I knew that the debugger had a fuzzy finder, I used this a. lot.) Manually putting debuggers in your code introduces a bunch of the problems inherent to Alert and Console, and while breakpoints aren&amp;#39;t visible to users who don&amp;#39;t have DevTools open, a full-window takeover looks way more terrifying than a line of console text.&lt;/p&gt;

&lt;p&gt;Debugging is better when a) it‚Äôs local; and b) it doesn‚Äôt require writing any code. If for some reason you need to actually code in &lt;code&gt;debuggers&lt;/code&gt; (or &lt;code&gt;console&lt;/code&gt;s or &lt;code&gt;alert&lt;/code&gt;s üò¨), look into making production vs. development configurations for your linter, and have it refuse to build if it finds any of that in an attempted production build.&lt;/p&gt;
&lt;h2 class='section-title' id=tldr&gt;&lt;a href='#tldr' class='section-inner'&gt;tl;dr&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You should start using the Chrome debugger today because:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Interactive debugging will help you work faster.&lt;/strong&gt; I‚Äôve lost way too much time including a typo in my &lt;code&gt;console.log&lt;/code&gt;s, or straight up logging something incorrect or unhelpful. The Chrome debugger lets you inspect code arbitrarily, which means less guessing, coding, refreshing, and waiting.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive debugging will help you understand your code.&lt;/strong&gt; Having the power to dive into your work on a line-by-line basis helps you understand not only what happens to specific values but how your entire project fits together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive debugging will prepare you for other programming environments.&lt;/strong&gt; My intro to interactive debugging didn‚Äôt involve Chrome but Xcode, during that strange year that I was a web/iOS developer. If you&amp;#39;re a frontend developer and might potentially want to do any other kind of programming, being familiar with interactive debugging will be massively helpful.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Getting comfortable with the Chrome debugger made me a much happier developer, and I really don‚Äôt know how I survived without it üò≠ Start playing around with it ASAP ‚Äî it may be overwhelming at first, but keep with it and it‚Äôll swiftly become your preferred way to work.&lt;/p&gt;
&lt;h2 class='section-title' id=parting-thoughts&gt;&lt;a href='#parting-thoughts' class='section-inner'&gt;Parting thoughts&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have a few more thoughts on the Chrome debugger that didn&amp;#39;t fit in the main narrative of this post, so enjoy:&lt;/p&gt;
&lt;h3 tabIndex=0&gt;The learning curve&lt;/h3&gt;
&lt;p&gt;There is absolutely a learning curve to using the Chrome debugger. You&amp;#39;ll have to do a lot of playing around with setting breakpoints and inspecting code, but as you work, keep asking yourself:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is this line of code actually executed by the browser?&lt;/li&gt;
&lt;li&gt;is this variable in scope right now?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may have trouble if the answer to either of these questions is &amp;quot;no&amp;quot;.&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Going deeper&lt;/h3&gt;
&lt;p&gt;I didn&amp;#39;t cover everything that you can do with the debugger. You should take your time to read more on it and play around with it. One fab thing, which you may have already guessed is possible, is that you can actually &lt;strong&gt;change the value of local variables&lt;/strong&gt; while on a breakpoint with that variable in scope. Set its value to whatever you want in the console.&lt;/p&gt;

&lt;p&gt;My point is, the debugger is amazing. Keep poking around and you&amp;#39;ll find even more cool stuff!&lt;/p&gt;
&lt;h3 tabIndex=0&gt;Backend JS debugging&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/nodejs/node/pull/6792"&gt;As of May 2016&lt;/a&gt;, you can freakin debug your Node.js code in Chrome. Paul Irish did &lt;a href="https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27"&gt;a lovely writeup&lt;/a&gt; on how to get this going. üöÄ&lt;/p&gt;

&lt;p&gt;Okay okay I&amp;#39;m done now. Happy coding!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Classifying Tweets with Keras and TensorFlow</title>
    <link rel="alternate" href="https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/"/>
    <id>https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/</id>
    <published>2017-09-02T19:05:00-07:00</published>
    <updated>2018-07-03T11:21:36-07:00</updated>
    <author>
      <name>Violet Pe√±a</name>
    </author>
    <summary type="html">I had a week to make my first neural network. I dove into TensorFlow and Keras, and came out with a deep neural network, trained on tweets, that can classify text sentiment. Here's an introduction to neural networks and machine learning, and step-by-step instructions of how to do it yourself.</summary>
    <content type="html">&lt;p&gt;Summer is drawing to a close. The air is humid and still. You‚Äôre between projects at work. What do you do with these few empty days?&lt;/p&gt;

&lt;p&gt;If you‚Äôre like me, you train a neural net. It had been on my ‚ÄúTo Do‚Äù list for about a year now, and while I had done some reading and tutorials, I hadn‚Äôt yet made my own from the ground up.&lt;/p&gt;

&lt;p&gt;I spent a few days chasing dead-ish ends, doing even more tutorials, and tinkering. I emerged with a custom neural net that could classify text as positive or negative with 79.3% accuracy.&lt;/p&gt;

&lt;p&gt;Here‚Äôs how to create your own neural net using Python, TensorFlow, and Keras! Happy learnings ‚ú®&lt;/p&gt;
&lt;h2 class='section-title' id=so-what-do-neural-nets-do&gt;&lt;a href='#so-what-do-neural-nets-do' class='section-inner'&gt;So what do neural nets do?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Neural net[work]s are collections of nodes that apply transformations to data. Their core behavior is: given an input, generate an output.&lt;/p&gt;

&lt;p&gt;The intriguing aspect of neural nets is that we don‚Äôt tell them &lt;em&gt;how&lt;/em&gt; to generate that output. Rather, we set them up to ‚Äúlearn‚Äù how to generate outputs based on massive amounts of training data. Training data consists of an input and an output, usually labelled by humans. The neural net intakes a piece of training data, generates an output, compares that output to the actual result, and adjusts the weights on its nodes ‚Äî that is, how likely an individual node is to return a certain intermediate value. Modifying these weights leads a net to return one value or another given an input, and is how the net refines its accuracy as it trains.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/nnet.png" alt="A diagram of a neural network with two hidden layers in addition to input and output layers." title="A sample neural network." width="597px" height="324px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A neural network with two hidden layers. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Nodes are arranged in layers within the neural net. All neural nets have an input layer and an output layer; within, they have at least one additional ‚Äúhidden‚Äù layer. ‚ÄúDeep‚Äù neural nets have more than one hidden layer. This is also the difference, name-wise, between ‚Äúmachine‚Äù and ‚Äúdeep‚Äù learning.
Once you train a neural net, it contains a fairly accurate self-adjusted system for creating outputs. Ideally, you can feed novel data into the net and end up with a meaningful output.&lt;/p&gt;

&lt;p&gt;Neural nets can either &lt;em&gt;classify&lt;/em&gt; extant data or &lt;em&gt;predict&lt;/em&gt; new data. &lt;a href="https://www.technologyreview.com/s/419223/using-neural-networks-to-classify-music/"&gt;Identifying musical genre&lt;/a&gt; is an example of the former; &lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html"&gt;fusing visual styles&lt;/a&gt;, of the latter. Kristen Stewart -- yes, the lead from &lt;em&gt;Twilight&lt;/em&gt; -- used this to give her film &lt;em&gt;Come Swim&lt;/em&gt; (2017) an impressionist look. She even &lt;a href="https://arxiv.org/pdf/1701.04928v1.pdf"&gt;co-authored a paper&lt;/a&gt; about the technique üî•üî•üî•&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-primary"&gt;
    &lt;img src="/images/keras/come-swim.png" alt="Four squares illustrating different degrees of visual treatment for &amp;quot;Come Swim&amp;quot;. All depict a man in water, but each has been modified to have its own visual style." title="Details from &amp;quot;Come Swim&amp;quot;" width="1404px" height="797px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Fine-tuning the visual treatment for &lt;em&gt;Come Swim&lt;/em&gt;. &lt;a href="https://arxiv.org/pdf/1701.04928v1.pdf"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html"&gt;Deep Dream&lt;/a&gt;, a project out of Google, both classifies and predicts. It uses classifications to suggest predictions, which in turn amplify the certainty of classifications. It turns landscapes and portraits into fields of roiling curves often resembling human eyes.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/deep-dream.jpg" alt="&amp;quot;The Scream&amp;quot; by Edvard Munch, but all the fields of color have been subdivided into roiling curls resembling nested eyes, cars, and dogs." title="A sample Deep Dream project" width="767px" height="965px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A sample of what Deep Dream can do when applied to Edvard Munch&amp;#39;s &lt;em&gt;The Scream&lt;/em&gt;. &lt;a href="https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ/photo/AF1QipPAcXwHkI3k8Sqnq-3WJUfXCZR68rZYTiR2b3te?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I won‚Äôt get into the specifics of neural nets or training/adjustment mechanics here. I‚Äôve done some reading but I have a long way to go before I am truly 1337. I highly recommend &lt;em&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/"&gt;Neural Networks and Deep Learning&lt;/a&gt;&lt;/em&gt; if you‚Äôre looking for a great intro to the field.&lt;/p&gt;
&lt;h2 class='section-title' id=whats-emourem-neural-net-going-to-do&gt;&lt;a href='#whats-emourem-neural-net-going-to-do' class='section-inner'&gt;What‚Äôs &lt;em&gt;our&lt;/em&gt; neural net going to do?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We‚Äôll perform the relatively straightforward task of classifying text ‚Äî specifically, we‚Äôll predict whether text expresses a positive or a negative sentiment. As training data, we‚Äôre using the behemoth &lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;Twitter Sentiment Analysis Dataset&lt;/a&gt; documented at ThinkNook.&lt;/p&gt;
&lt;h2 class='section-title' id=getting-started-environment-and-tools&gt;&lt;a href='#getting-started-environment-and-tools' class='section-inner'&gt;Getting started: environment and tools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Our neural net is Python 2 based, so make sure that‚Äôs what you‚Äôre working with on your own machine. I highly recommend &lt;a href="https://virtualenv.pypa.io/en/stable/"&gt;virtualenv&lt;/a&gt; for managing your package installs, and &lt;a href="https://ipython.org/"&gt;IPython&lt;/a&gt; as an interactive editor.
The net itself will be built using &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow&lt;/a&gt;, an open-source, Google-backed machine learning framework. We‚Äôre laying &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; on top of TensorFlow to act as an API and simplify TensorFlow‚Äôs syntax.
If you want to dig into TensorFlow on its own for a bit, &lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;their ‚ÄúFor Beginners‚Äù tutorial&lt;/a&gt; is informative and surprisingly painless.&lt;/p&gt;
&lt;h2 class='section-title' id=language-and-machines&gt;&lt;a href='#language-and-machines' class='section-inner'&gt;Language and machines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For me, there are few joys on par with working with natural language. In machine learning, we have two ways of representing language language: vector embeddings or one-hot matrices.
Vector embeddings are spatial mappings of words or phrases. Relative locations of words indicate similarity and suggest semantic relationships ‚Äî for instance, &lt;a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;vector embeddings can be used to generate analogies&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/vectors.png" alt="Graphs depict vector embeddings -- circles with lines drawn between them to convey analogous relationships between the words represented by the circles." title="A sample neural network." width="1576px" height="772px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;Sample vector embeddings that demonstrate analogous relationships between words. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;One-hot matrices, on the other hand, contain no linguistic information. (If you‚Äôre taking all the recommended detours, you‚Äôll remember one-hot matrices from the &lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;TensorFlow MNIST digit classification tutorial&lt;/a&gt;.) They‚Äôre na√Øve; they indicate what data they contain but suggest nothing &lt;em&gt;about&lt;/em&gt; that data, or its relationship to other information.
I decided to use one-hot matrices so that I could focus on other aspects of the other project. So let‚Äôs talk about them for a bit.&lt;/p&gt;
&lt;h2 class='section-title' id=enter-the-one-hot-matrix&gt;&lt;a href='#enter-the-one-hot-matrix' class='section-inner'&gt;Enter the (one-hot) matrix&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One-hot matrices are called ‚Äúone-hot‚Äù because they each embody one dimension of difference from each other; each matrix has one distinguishing (‚Äúhot‚Äù) characteristic. We can take all the data in our system and represent them using this flattened system.
As an example, let‚Äôs take a couple of lines from &lt;a href="https://www.python.org/dev/peps/pep-0020/"&gt;PEP 20&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Complex is better than complicated.
Flat is better than nested.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;How do we represent that in a one-hot matrix?&lt;/p&gt;

&lt;p&gt;We begin by tokenizing the utterance; that is, breaking it into words. We end up with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'complex'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'complicated'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'flat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'nested'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can create a lookup dictionary of all  the unique words. We now have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s"&gt;'complex'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'is'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'better'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'than'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'complicated'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'flat'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;'nested'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Count doesn‚Äôt matter; order doesn‚Äôt matter. Each token just needs a unique identifier.
Now to create the matrices: each token needs to be transformed from a string into an array. Each array is of the length of the dictionary, and each value in the dictionary that‚Äôs &lt;em&gt;not&lt;/em&gt; the value of the current token is represented by a 0. The value of the token is represented with a 1. ‚ÄúComplex is better than complicated‚Äù would look like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#complex&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#is&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#better&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#than&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c"&gt;#complicated&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now we can deal with the tokens in a uniform way, since they‚Äôre all represented by isomorphic data structures, and once we‚Äôre done we can look up their values using the dictionary we made earlier.&lt;/p&gt;

&lt;p&gt;One-hot matrices get large quickly. In my example, I‚Äôm ‚Äúonly‚Äù using the top 3000 most-commonly occurring words in the training corpus. This means that each word becomes represented by an array 3000 items long üò¨ Whether using one-hot matrices or not, we‚Äôre reckoning with a ton of data, but one-hot matrices are ideally used for a small or finite dataset. In MNIST, for example, a one-hot matrix is used to encode information about whether an image represents a digit from 0 to 9. All the arrays are kept nice and small to a length of 10.&lt;/p&gt;
&lt;h2 class='section-title' id=lets-get-cooking&gt;&lt;a href='#lets-get-cooking' class='section-inner'&gt;Let‚Äôs get cooking&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Enough preamble; time to get started actually building our neural net! Our work will be based on &lt;a href="https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py"&gt;the Reuters example&lt;/a&gt; in the Keras github repo, but we‚Äôll use our own data set and make a couple more tweaks on the way.&lt;/p&gt;

&lt;p&gt;I will be going over all the code in detail, but &lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;I have published it in full in a gist&lt;/a&gt;. It doesn&amp;#39;t have a ton of backstory but it does have all the code in one nice place for you.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Neural nets can take anywhere from a few moments to days to train, depending on your hardware and on how large and/or complex your dataset is. This net took me ~60 minutes to train on a mid-2015 MacBook Pro (and it got NOISY üòÖ ). My point is, you probably won‚Äôt want to have to train the net every single time you want to use it. The last step of our training script will also save the net so that we can ‚Äúboot it up‚Äù quickly from another script when we actually want to consult it.&lt;/p&gt;

&lt;p&gt;Let‚Äôs worry about the training script first. We‚Äôll need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Get our data into a usable format&lt;/li&gt;
&lt;li&gt;Build our neural net&lt;/li&gt;
&lt;li&gt;Train it with said data&lt;/li&gt;
&lt;li&gt;Save the neural net for future use&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class='section-title' id=organizing-our-data&gt;&lt;a href='#organizing-our-data' class='section-inner'&gt;Organizing our data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We‚Äôre using the &lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;Twitter Sentiment Analysis Dataset&lt;/a&gt; available via ThinkNook. Be prepared; this dataset is &lt;em&gt;extremely large&lt;/em&gt; and may take forever-ish to open in Excel (I had more success with Numbers).&lt;/p&gt;

&lt;p&gt;One you open it, you can see a massive table that starts with this:&lt;/p&gt;
&lt;div class='table-wrap'&gt;&lt;table&gt;&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;ItemID&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sentiment&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;SentimentSource&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;SentimentText&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;is so sad for my APL friend.............&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;I missed the New Moon trailer...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Sentiment140&lt;/td&gt;
&lt;td&gt;omg its already 7:30 :O&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;The only columns we‚Äôre interested in here are 1 and 3 ‚Äî we‚Äôll be training our net on inputs of column &lt;code&gt;SentimentText&lt;/code&gt; with outputs of &lt;code&gt;Sentiment&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We need to convert &lt;code&gt;SentimentText&lt;/code&gt; utterances to one-hot matrices, and create a dictionary of all the words we keep track of. Here, the &lt;code&gt;numpy&lt;/code&gt; library is your friend. I hadn‚Äôt used it much before this but it‚Äôs super powerful and has some really useful built-in utilities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;

&lt;span class="c"&gt;# extract data from a csv&lt;/span&gt;
&lt;span class="c"&gt;# notice the cool options to skip lines at the beginning&lt;/span&gt;
&lt;span class="c"&gt;# and to only take data from certain columns&lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'path/to/your/data.csv'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;','&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;usecols&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# create our training data from the tweets&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;# index all the sentiment labels&lt;/span&gt;
&lt;span class="n"&gt;train_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Okay, we‚Äôve indexed all of our data; time to use Keras to make it machine-friendly.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;

&lt;span class="c"&gt;# only work with the 3000 most popular words found in our dataset&lt;/span&gt;
&lt;span class="n"&gt;max_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3000&lt;/span&gt;

&lt;span class="c"&gt;# create a new Tokenizer&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;max_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# feed our tweets to the Tokenizer&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_on_texts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Tokenizers come with a convenient list of words and IDs&lt;/span&gt;
&lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_index&lt;/span&gt;
&lt;span class="c"&gt;# Let's save this out so we can use it later&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'dictionary.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# one really important thing that `text_to_word_sequence` does&lt;/span&gt;
    &lt;span class="c"&gt;# is make all texts the same length -- in this case, the length&lt;/span&gt;
    &lt;span class="c"&gt;# of the longest text in the set.&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_to_word_sequence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;allWordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="c"&gt;# for each tweet, change each token to its ID in the Tokenizer's word_index&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;wordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordIndices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# now we have a list of all tweets converted to index arrays.&lt;/span&gt;
&lt;span class="c"&gt;# cast as an array for future usage.&lt;/span&gt;
&lt;span class="n"&gt;allWordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# create one-hot matrices out of the indexed tweets&lt;/span&gt;
&lt;span class="n"&gt;train_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sequences_to_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allWordIndices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'binary'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# treat the labels as categories&lt;/span&gt;
&lt;span class="n"&gt;train_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;‚ú® Cooooooooooooool. ‚ú® Now you have training data and labels that you‚Äôll be able to pipe right into your neural net.&lt;/p&gt;
&lt;h2 class='section-title' id=making-a-model&gt;&lt;a href='#making-a-model' class='section-inner'&gt;Making a model&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Keras makes building neural nets as simple as possible, to the point where you can add a layer to the network in short line of code. Here‚Äôs how I built my net:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.layers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Activation&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_words&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'softmax'&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looks simple enough. What does it mean?? üåàüåà&lt;/p&gt;

&lt;p&gt;Keras‚Äô &lt;code&gt;Sequential()&lt;/code&gt; is a simple type of neural net that consists of a ‚Äústack‚Äù of layers executed in order.&lt;/p&gt;

&lt;p&gt;If we wanted to, we could make a stack of only two layers (input and output) to make a complete neural net ‚Äî without hidden layers, it wouldn‚Äôt be considered a deep neural net.&lt;/p&gt;

&lt;p&gt;The input and output layers are the most important, since they determine the overall shape of the neural net. You need to know what kind of input to expect, and what kind of output you want.&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/nnet2.png" alt="A diagram of a neural net used to identify digits using MNIST data." title="A page from the Perspectiva Corporum Regularium" width="537px" height="447px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;A representation of a neural net for identifying digits using the MNIST dataset. &lt;a href="http://neuralnetworksanddeeplearning.com/chap1.html"&gt;Source&lt;/a&gt;&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Out network will mostly consist of &lt;code&gt;Dense&lt;/code&gt; layers ‚Äî the ‚Äústandard‚Äù, linear neural net layer of inputs, weights, and outputs.&lt;/p&gt;

&lt;p&gt;In our case, we‚Äôre inputting a sentence that will be converted to a one-hot matrix of length &lt;code&gt;max_words&lt;/code&gt; ‚Äî here, 3000. We also include how many outputs we want to come out of that layer (512, for funsies) and what kind of maximization (or ‚Äúactivation‚Äù) function to use.&lt;/p&gt;

&lt;p&gt;Activation functions are used when training the network; they tell the network how to judge when a weight for a particular node has created a good fit. In the first layer, I use &lt;code&gt;relu&lt;/code&gt; (also for funsies). Activation functions differ, mostly in speed, but all the ones available in Keras and TensorFlow are viable; feel free to play around with them. If you don‚Äôt explicitly add an activation function, that layer will use a linear one.&lt;/p&gt;

&lt;p&gt;Our output layer consists of two possible outputs, since that‚Äôs how many categories our data could get sorted into. If you use a neural net to predict rather than classify, you‚Äôre actually creating a neural net with one possible output ‚Äî the prediction.&lt;/p&gt;

&lt;p&gt;In between the input and output layers, we have one more &lt;code&gt;Dense&lt;/code&gt; layer and two &lt;code&gt;Dropout&lt;/code&gt; layers. Dropouts are used to randomly remove data, which can help avoid overfitting. Overfitting can happen when you keep training on the same or overly-similar data ‚Äî as you train, your accuracy will hold steady or drop instead of rising.&lt;/p&gt;

&lt;p&gt;As the last step before training, we need to compile the network:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'adam'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'accuracy'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Specifying that we want to collect the &lt;code&gt;accuracy&lt;/code&gt; metric will give us really helpful live output as we train our model.&lt;/p&gt;
&lt;h2 class='section-title' id=how-to-train-your-network&gt;&lt;a href='#how-to-train-your-network' class='section-inner'&gt;How to train your network&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is some tiny code that will take a while to run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;validation_split&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We‚Äôre fitting (training) our model off of inputs &lt;code&gt;train_x&lt;/code&gt; and categories &lt;code&gt;train_y&lt;/code&gt;. We evaluate data in groups of &lt;code&gt;batch_size&lt;/code&gt;, checking the network‚Äôs accuracy, tweaking node weights, and then running through another batch. Small batches let you train networks much more quickly than if you tried to use a batch the size of your entire training dataset.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;epochs&lt;/code&gt; is how many times you do this batch-by-batch splitting. I‚Äôve found 5 to be good in this case; I tried 7, but ended up overfitting.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;validation_split&lt;/code&gt; says how much of your input you want to be reserved for testing data ‚Äî essential for seeing how accurate your network is at that point. Recommended training-to-test ratios are 80:20 or 90:10. You don‚Äôt want to compromise the size of your training corpus, but you need enough test data to actually see how your net is doing.&lt;/p&gt;

&lt;p&gt;Now go get coffee, or maybe a meal. And if you‚Äôre on a laptop, make sure it‚Äôs plugged in ‚Äî training can be a real battery killer ‚ò†Ô∏è&lt;/p&gt;

&lt;p&gt;&lt;figure role="img" class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/perspectiva.jpg" alt="An etching of a fractured geometric figure propped up on a platform surrounded by quare pyramids and a crucifix." title="A page from the Perspectiva Corporum Regularium" width="1191px" height="800px" /&gt;
  &lt;figcaption&gt;
    &lt;p&gt;If you need something to lose yourself in for about an hour, I suggest the &lt;em&gt;&lt;a href="http://digital.slub-dresden.de/werkansicht/dlf/12830/"&gt;Perspectiva Corporum Regularium&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;As you train the neural net, Keras will output running stats on what epoch you‚Äôre in, how much time is left in that epoch of training, and current accuracy. The value to watch is not &lt;code&gt;acc&lt;/code&gt; but &lt;code&gt;val_acc&lt;/code&gt;, or Validation Accuracy. This is your neural net&amp;#39;s score when predicting values for data in your validation split.&lt;/p&gt;

&lt;p&gt;Your accuracy should start out low per epoch and rise throughout the epoch; it should increase at least a little across epochs. If your accuracy starts decreasing, you‚Äôre overfitting.&lt;/p&gt;

&lt;p&gt;This is some sample output from training using this code over five epochs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;780&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4947&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7610&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4500&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7884&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;850&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4737&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7760&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4481&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7902&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;788&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4662&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7817&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4446&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7921&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;819&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4607&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7859&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4471&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7921&lt;/span&gt;
&lt;span class="n"&gt;Epoch&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="mi"&gt;1420764&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;1420764&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==============================&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;829&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4569&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7887&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.4439&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.7927&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our accuracy increases from 78.8% accurate by the end of Epoch 1 to 79.3% at the end of Epoch 5.&lt;/p&gt;

&lt;p&gt;&lt;div class="image-wrap image-primary"&gt;
    &lt;img src="/images/keras/noice.gif" alt="NOICE!" title="NOICE!!" width="500px" height="201px" /&gt;
  &lt;/div&gt;&lt;/p&gt;
&lt;h2 class='section-title' id=saving-your-model&gt;&lt;a href='#saving-your-model' class='section-inner'&gt;Saving your model&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Once you‚Äôre done training, it‚Äôs time to save your net so that you don‚Äôt have to keep repeating all of those steps.&lt;/p&gt;

&lt;p&gt;Your model gets saved in two parts: One is the model‚Äôs structure itself; the other is the weights used in those model‚Äôs nodes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="n"&gt;model_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'w'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.h5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that‚Äôs it!&lt;/p&gt;
&lt;h2 class='section-title' id=party-time&gt;&lt;a href='#party-time' class='section-inner'&gt;Party time&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Finally you can use your neural net! In a new file, you‚Äôll open the model, its weights, and your dictionary, and then put those together to classify text. We‚Äôre going to go over the whole file at once because I‚Äôm excited to actually use it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.preprocessing.text&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;model_from_json&lt;/span&gt;

&lt;span class="c"&gt;# we're still going to use a Tokenizer here, but we don't need to fit it&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_words&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# for human-friendly printing&lt;/span&gt;
&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'negative'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'positive'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c"&gt;# read in our saved dictionary&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'dictionary.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# this utility makes sure that all the words in your input&lt;/span&gt;
&lt;span class="c"&gt;# are registered in the dictionary&lt;/span&gt;
&lt;span class="c"&gt;# before trying to turn them into a matrix.&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kpt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_to_word_sequence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;wordIndices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;wordIndices&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"'&lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;s' not in training corpus; ignoring."&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wordIndices&lt;/span&gt;

&lt;span class="c"&gt;# read in your saved model structure&lt;/span&gt;
&lt;span class="n"&gt;json_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.json'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loaded_model_json&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;json_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;# and create a model from that&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_from_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loaded_model_json&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# and weight your nodes with your saved values&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'model.h5'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# okay here's the interactive part&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;evalSentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;raw_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'Input a sentence to be evaluated, or Enter to quit: '&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evalSentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;

    &lt;span class="c"&gt;# format your input for the neural net&lt;/span&gt;
    &lt;span class="n"&gt;testArr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;convert_text_to_index_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evalSentence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sequences_to_matrix&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;testArr&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'binary'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;# predict which bucket your input belongs in&lt;/span&gt;
    &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;# and print it for the humons&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;s sentiment; &lt;/span&gt;&lt;span class="si"&gt;%&lt;/span&gt;&lt;span class="s"&gt;f&lt;/span&gt;&lt;span class="si"&gt;%% &lt;/span&gt;&lt;span class="s"&gt;confidence"&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you run this file, you create a new model using the saved structure, and then you get a little command prompt for input to classify.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;model.predict()&lt;/code&gt; takes what you give it, runs it through the trained neural net, and gives you a reading of how confident it is that that input belongs in each output bucket. In our case, we have two outputs, so we have two confidence estimations that range from 0 to 1; whichever one is higher is the network‚Äôs ultimate classification of that data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: It's alive! :D
positive sentiment; 80.760884% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;üëåüèª&lt;/p&gt;
&lt;h2 class='section-title' id=you-did-it&gt;&lt;a href='#you-did-it' class='section-inner'&gt;You did it!&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;You‚Äôve trained your first neural net that evaluates text as expressing positive or negative sentiment. And it works well(ish):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: That went better than expected
positive sentiment; 56.355631% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: That did not go as expected
negative sentiment; 86.936867% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 class='section-title' id=next-steps&gt;&lt;a href='#next-steps' class='section-inner'&gt;Next steps&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;div class="image-wrap image-secondary"&gt;
    &lt;img src="/images/keras/party.gif" alt="" title="Thanks for sticking with me through all of that." width="480px" height="360px" /&gt;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;This is a really basic neural net, and there‚Äôs a lot more I‚Äôd like to investigate. Among other things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You can deploy machine learning models to the cloud using &lt;a href="https://cloud.google.com/ml-engine/"&gt;Google Cloud ML&lt;/a&gt; ‚Äî what could I do if I ported my local machine to operate on Google Cloud Platform, or if I started building new models using that service?&lt;/li&gt;
&lt;li&gt;I chose the option of less-powerful word indexing by using one-hot matrices instead of vector embeddings. What accuracy could I get if I started using the latter?&lt;/li&gt;
&lt;li&gt;Classification is great but I‚Äôd really love to work on text &lt;em&gt;generation&lt;/em&gt;, guessing at the next most-likely word in the sequence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of the code in this post, plus the requirements file, is up &lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;in a Github gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Happy coding! And thanks for following me on this journey üåü&lt;/p&gt;
&lt;h2 class='section-title' id=coda-how-well-does-it-work-and-more-on-data&gt;&lt;a href='#coda-how-well-does-it-work-and-more-on-data' class='section-inner'&gt;Coda: How well does it work? And more on data&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I‚Äôm super proud of this neural net but 79% accuracy means that it‚Äôs wrong 21% of the time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: I wish I could show you when you are lonely or in darkness the astonishing light of your own being
negative sentiment; 87.021768% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre class="code plaintext"&gt;&lt;code&gt;Input a sentence to be evaluated, or Enter to quit: foo bar
positive sentiment; 62.751633% confidence
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ü§î ü§î Those don‚Äôt look quite right.&lt;/p&gt;

&lt;p&gt;We can always build better models, but a lot of this is &lt;a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out"&gt;Garbage In, Garbage Out&lt;/a&gt;. The dataset I used is incredibly large, but looking through it, there are classifications I don‚Äôt agree with, such as marking &lt;code&gt;... health class (what a joke!)&lt;/code&gt; as positive. Without bringing compensation algorithms into the mix, your neural net can only be as accurate as your training data.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;On that note, I encourage you to examine your training data closely. What biases or prejudices might have influenced that information? Those biases will also be present, explicitly or implicitly, in your output.&lt;/p&gt;

      &lt;blockquote&gt;
        &lt;p&gt;The past is a very racist place. And we only have data from the past to train Artificial Intelligence.
        &lt;cite&gt;
          Trevor Paglen&lt;/p&gt;

        &lt;/cite&gt;
      &lt;/blockquote&gt;
    
&lt;p&gt;This doesn‚Äôt mean that any and all data are inherently biased and therefore unusable ‚Äî we just need to be aware of that bias and work to eradicate it. Researchers from Boston University and Microsoft have &lt;a href="https://arxiv.org/abs/1607.06520"&gt;created ways to work with appropriately gendered analogies without reinforcing gender stereotypes&lt;/a&gt;. &lt;a href="https://artificialintelligencenow.com/"&gt;AI Now&lt;/a&gt; is an entire research organization dedicated to teasing out the biases and impacts of artificial intelligence and machine learning.&lt;/p&gt;

&lt;p&gt;ML is powerful and we can make amazing things with it. Let‚Äôs use it to create a more equitable future.&lt;/p&gt;
&lt;h2 class='section-title' id=downloads&gt;&lt;a href='#downloads' class='section-inner'&gt;Downloads&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023"&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"&gt;dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
</feed>
